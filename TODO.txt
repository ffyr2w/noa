Missing features that could be useful to add, in no particular order.

cryoEM:
    -   CTF
        Add CTF class gathering all the necessary variables (voltage, Cs, etc.) and basic utility functions,
        e.g. compute value at a given frequency. Single side-band algorithm support (complex CTFs, cones).
        Add related functions in `filter::`, e.g. compute CTF with a given shape, etc.?

    -   High-order aberrations and optics in general...
        Look at Warp and RELION's code.

    -   Familiarise myself with: optimization/minimization/fitting/refinement, gradients, etc.
        Also solve linear equations, look at robust statistics as well.
        This would mostly be an interface to other libraries, e.g. L-BFGS.

    -   Add center of mass.
    -   Add (conical) FSC
    -   Add real-space binning

    -   Add compression for `ImageFile` and `io::` in general.

    -   MRC files and complex types ala IMOD.
        Atm, ImageFile will treat complex data as real data. However, IMOD excepts the non-redundant shape.
        I should contact David about this. IMOD saves the logical shape but always excepts the non-redundant data.
        ImageFile will treat the shape as the physical shape, so currently we have to set the shape to the physical
        shape, write the redundant array and then reset to the logical shape...

Development:
    -   Filter and View.
        The ewise and indexing functionalities are still quite limited. For instance, to reproduce the Numpy a[b>0]=1
        functionality, we need to create a sequence of indexes (see memory::extract()) or a map (see math::ewise()).
        While this can be the best solution in some cases, it is often not efficient.
        An alternative is to add a Filter type, allowing to create a nice API without losing any performance. Filter
        is just an object keeping tracking of the operands (the View(s)) and (unary|binary|trinary) operator. Then
        functions like ewise(), set(), insert() can take this Filter and evaluate in an element-wise manner the operator
        to know whether this index is included or not.
        For instance:
        const auto filter = where(b, 1, greater_t{}); // (b > 1) no computation, lazy evaluated
        memory::set(a, 1, filter);

    -   Unified API.
        Atm users interact directly with the backend. They cannot easily write "device" agnostic code. The unified
        API is simply an interface for the users to use and switch between backends at runtime.

    -   Update projectBackward/Forward. Move them to reconstruct::fft.

    -   Add nvrtc when necessary.
        Atm, nvcc and the CUDA runtime are used to compile kernels. The idea is to use nvrtc to compile (some) kernels
        to cubin directory at runtime. The launch mechanism will be centralized and in .cpp files, the kernels in
        .cu files. Ultimately, this will make us use the driver API, which gets rid of the triple-chevron and offers
        better control over the parameter packing and kernel launch.
        This will be the first step toward JIT and fused kernels for the CUDA backend.
        -   Note that the simple fact of having kernels and the launch in separate files is already beneficial.
            For instance, we could refactor the code so that nvcc won't have to see and compile fmt/spdlog.
        -   One other major optimization allowed by runtime compilation is the ability to bring some runtime evaluation
            to compile time. For instance, in some cases, passing some dimensions or count as a template parameter
            could be beneficial. Of course this needs to be tuned since we don't want to recompile everytime the
            function is called because there's a different template parameter...
        See: https://github.com/arrayfire/arrayfire/blob/master/src/backend/cuda/compile_module.cpp

    -   Test CUDA LTO and unused kernels support from 11.5.

    -   Add proper benchmarks. This is more important that it seems, so do it.

    -   For transformation, test the benefits of constant memory. The issue with this is the (host) thread-safety.
        If multiple host threads using the same device and different streams, we need to lock guard the resource and
        kernel launch.

    -   Add `ewise` with indexes.
        On the CPU, this is quite easy and already supported by the STL. On CUDA, this is more difficult.
        Look at Thrust implementation or Cupy. Its usefulness decreases if we add a proper JIT/kernel fusion,
        but it could be the base of the JIT system as well.

    -   JIT, lazy evaluation and kernel fusion.
        This is cool. The ability to fuse kernels together is great, but if we have a `for_each` working
        on the CPU and GPU, this is less useful (same performance but JIT is creating the `for_each` being the scene
        so simpler for the users). The implementation in arrayfire seems fine, I think Cupy is similar but CUDA only.
        Pytorch should be the same but the codebase is huge, and it's difficult to understand what is going on.
        Links: https://arrayfire.com/performance-of-arrayfire-jit-code-generation/

    -   Add vkFFT support? Also look at FFTW CPU port for Intel and AMD.
        Zero-padding can bring up to 2x increase of performance for 2D, 3x for 3D. Convolution can be added as a call-
        back but there's no benchmark for that. It could be useful to contact the author about some of our applications,
        e.g. convolution with small (compared to input) template. FastFFT from Ben Himes is also promising since it is
        really fitted for cryoEM applications, however it is CUDA only, which is a huge drawback.

    -   Add Vulkan backend.
        CUDA is nice and all, but it's NVIDIA only. Vulkan seems to be the best option for proper GPU support.
        GLSL for shading language and then glslangValidator or glslc to compile to SPIR-V.

    -   Add command line interface and create "mini"-programs, ala IMOD, as one way to use the library.
