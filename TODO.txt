Missing features that could be useful to add, in no particular order.

cryoEM:
    -   Reconstruction algorithm(s)?
        From what I've seen and read, people tend to back-project using Direct Fourier Insertion and then use
        a sinc correction to correct for the new polar griding. On top of that, RELION (and Warp) uses an
        iterative method using a Kaiser window for weighting. I'd need to look at this in more details...

    -   CTF
        Add CTF class gathering all the necessary variables (voltage, Cs, etc.) and basic utility functions,
        e.g. compute value at a given frequency. Single side-band algorithm support (complex CTFs, cones).
        Add related functions in `filter::`, e.g. compute CTF with a given shape, etc.?

    -   High-order aberrations and optics in general...
        Look at Warp and RELION's code.

    -   Familiarise myself with: optimization/minimization/fitting/refinement, gradients, etc.
        Also solve linear equations, look at robust statistics as well.
        This would mostly be an interface to other libraries, e.g. L-BFGS.

    -   Add center of mass.
    -   Add (conical) FSC
    -   Add real-space binning

    -   Add compression for `ImageFile` and `io::` in general.

    -   MRC files and complex types ala IMOD.
        Atm, ImageFile will treat complex data as real data. However, IMOD excepts the non-redundant shape.
        I should contact David about this. IMOD saves the logical shape but always excepts the non-redundant data.
        ImageFile will treat the shape as the physical shape, so currently we have to set the shape to the physical
        shape, write the redundant array and then reset to the logical shape...

Development:
    -   Unified API.
        Atm users interact directly with the backend. They cannot easily write "device" agnostic code. The unified
        API is simply an interface for the users to use and switch between backends at runtime. The `Array` class
        should have one pointer and keep track of the resource managing this pointer. So the resource (host, pinned,
        gpu, etc.) could be chosen at runtime and function calls will check this resource tag to redirect to the
        correct backend.

    -   Replace math arithmetics with `ewise` function.
    -   Add std::transform equivalent to support 3D pitch, streams and OpenMP.
    -   Add creation function (probably in memory), e.g. memory::ones()/arange(), etc.
    -   Add memory::cast(const T*, U*, clamp), which simply static_cast element-wise.

    -   Improve CUDA kernel launch and compilation mechanism.
        Atm, nvcc and the CUDA runtime are used to compile kernels to PTX. The idea is to use nvrtc to compile kernels
        to cubin directory at runtime. The launch mechanism will be centralized and in .cpp files, the kernels in
        .cu files. Ultimately, this will make us use the driver API, which gets rid of the triple-chevron and offers
        better control over the parameter packing and kernel launch.
        This will be the first step toward JIT and fused kernels for the CUDA backend.
        -   Note that the simple fact of having kernels and the launch in separate files is already beneficial.
            For instance, we could refactor the code so that nvcc won't have to see and compile fmt/spdlog.
        -   One other major optimization allowed by runtime compilation is the ability to bring some runtime evaluation
            to compile time. For instance, in some cases, passing some dimensions or count as a template parameter
            could be beneficial. Of course this needs to be tuned since we don't want to recompile everytime the
            function is called because there's a different template parameter...
        See: https://github.com/arrayfire/arrayfire/blob/master/src/backend/cuda/compile_module.cpp

    -   Add multithreading for the CPU.
        OpenMP vs oneTBB? oneTBB seems easier to deal with concerning shared data, but I think OpenMP is more
        robust and support a wider range of features (e.g. atomic add). Also, FFTW3 is compatible with OpenMP.
        -   The original idea was to use "execution policies" in the unified and CPU backend API. In any case, it is
            likely that a lot of refactoring will be necessary. Maybe centralize kernels launches to have one common
            interface and not #pragma omp everywhere...

    -   Replace STL nested exceptions by our own global mechanism.
        On construction, exceptions should be registered to a thread-safe global/static vector. what() will simply
        point to that vector. This is quite important for dynamic linking.

    -   Add CUB support for the CUDA backend.
        This is mostly for reductions...
reductions
    -   Add proper benchmarks. This is more important that it seems, so do it.

    -   For transformation, test the benefits of constant memory. The issue with this is the (host) thread-safety.
        If multiple host threads using the same device and different streams, we need to lock guard the resource and
        kernel launch.

    -   Add `for_each`.
        On the CPU, this is quite easy and already supported by the STL. On CUDA, this is more difficult.
        Look at Thrust implementation or Cupy. Its usefulness decreases if we add a proper JIT/kernel fusion,
        but it could be the base of the JIT system as well.

    -   JIT, lazy evaluation and kernel fusion.
        This is cool. The ability to fuse kernels together is great, but if we have a `for_each` working
        on the CPU and GPU, this is less useful (same performance but JIT is creating the `for_each` being the scene
        so simpler for the users). The implementation in arrayfire seems fine, I think Cupy is similar but CUDA only.
        Pytorch should be the same but the codebase is huge, and it's difficult to understand what is going on.
        Links: https://arrayfire.com/performance-of-arrayfire-jit-code-generation/

    -   Add vkFFT support? Also look at FFTW CPU port for Intel and AMD.
        Zero-padding can bring up to 2x increase of performance for 2D, 3x for 3D. Convolution can be added as a call-
        back but there's no benchmark for that. It could be useful to contact the author about some of our applications,
        e.g. convolution with small (compared to input) template. FastFFT from Ben Himes is also promising since it is
        really fitted for cryoEM applications, however it is CUDA only, which is a huge drawback.

    -   Add Vulkan backend.
        CUDA is nice and all, but it's NVIDIA only. Vulkan seems to be the best option for proper GPU support.
        GLSL for shading language and then glslangValidator or glslc to compile to SPIR-V.

    -   Add command line interface and create "mini"-programs, ala IMOD, as one way to use the library.
